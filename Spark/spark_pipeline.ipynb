{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = \"b746917e-ceb7-4ae0-81e6-3ccd893cb0de\"\n",
    "rg = \"dpv2\"\n",
    "workspace = \"dpv2-wks\"\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), sub_id, rg, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml.entities import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_kmeans = load_component(\"./components/spark_kmeans.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output = command(\n",
    "    inputs=dict(\n",
    "        spark_output=Input(\n",
    "            type=AssetTypes.URI_FOLDER\n",
    "        )\n",
    "    ),\n",
    "    command = \"ls ${{inputs.spark_output}}\",\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading components (0.0 MBs): 100%|##########| 2166/2166 [00:01<00:00, 2009.89it/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading sample_kmeans_data.txt\u001b[32m (< 1 MB): 100%|##########| 124/124 [00:00<00:00, 459B/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from msilib.schema import Directory\n",
    "\n",
    "\n",
    "@pipeline()\n",
    "def spark_pipeline(train_data):\n",
    "    kmeans_clustering_step = spark_kmeans(file_input = train_data)\n",
    "    kmeans_clustering_step.compute = \"spark-cluster\"\n",
    "    kmeans_clustering_step.outputs.output.mode = InputOutputModes.DIRECT\n",
    "\n",
    "    show_output_step = show_output(spark_output = kmeans_clustering_step.outputs.output)\n",
    "\n",
    "sample_data = Input(\n",
    "    path=\"./data/sample_kmeans_data.txt\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=InputOutputModes.DIRECT\n",
    ")\n",
    "\n",
    "job = spark_pipeline(train_data = sample_data)\n",
    "\n",
    "job.settings.default_compute  = \"cpu-cluster\"\n",
    "\n",
    "ret_job = ml_client.jobs.create_or_update(job, experiment_name=\"spark_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: silver_stick_5wjg26tn4f\n",
      "Web View: https://ml.azure.com/runs/silver_stick_5wjg26tn4f?wsid=/subscriptions/b746917e-ceb7-4ae0-81e6-3ccd893cb0de/resourcegroups/dpv2/workspaces/dpv2-wks\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2022-10-06 06:34:16Z] Submitting 1 runs, first five are: e5be719d:802ee6f9-cff3-4be5-8750-4553b9be703c\n",
      "[2022-10-06 06:41:55Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: silver_stick_5wjg26tn4f\n",
      "Web View: https://ml.azure.com/runs/silver_stick_5wjg26tn4f?wsid=/subscriptions/b746917e-ceb7-4ae0-81e6-3ccd893cb0de/resourcegroups/dpv2/workspaces/dpv2-wks\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has some failed steps. See child run or execution logs for more details.\",\n        \"message_format\": \"Pipeline has some failed steps. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus\",\n    \"location\": \"eastus\",\n    \"time\": \"2022-10-06T06:41:56.060864Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\dpv2demo\\Spark\\spark_pipeline.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/code/dpv2demo/Spark/spark_pipeline.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mstream(ret_job\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\tangm\\anaconda3\\envs\\dpv2\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:260\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    259\u001b[0m     \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 260\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tangm\\anaconda3\\envs\\dpv2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:548\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[39m=\u001b[39mjob_object\u001b[39m.\u001b[39mid)\n\u001b[0;32m    547\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 548\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_logs_until_completion(\n\u001b[0;32m    549\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runs_operations, job_object, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datastore_operations, requests_pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_pipeline\n\u001b[0;32m    550\u001b[0m     )\n\u001b[0;32m    551\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tangm\\anaconda3\\envs\\dpv2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:297\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    295\u001b[0m         file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m         \u001b[39mraise\u001b[39;00m JobException(\n\u001b[0;32m    298\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(json\u001b[39m.\u001b[39mdumps(error, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)),\n\u001b[0;32m    299\u001b[0m             target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mJOB,\n\u001b[0;32m    300\u001b[0m             no_personal_data_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException raised on failed job.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    301\u001b[0m             error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mSYSTEM_ERROR,\n\u001b[0;32m    302\u001b[0m         )\n\u001b[0;32m    304\u001b[0m file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    305\u001b[0m file_handle\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has some failed steps. See child run or execution logs for more details.\",\n        \"message_format\": \"Pipeline has some failed steps. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"eastus\",\n    \"location\": \"eastus\",\n    \"time\": \"2022-10-06T06:41:56.060864Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "ml_client.jobs.stream(ret_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dpv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd93e304daff9e4e686bfc9607973de377fed28736a52eb30bf8c94fedb78ed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
