{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml import command, Input, Output, load_component\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = \"b746917e-ceb7-4ae0-81e6-3ccd893cb0de\"\n",
    "rg = \"dpv2\"\n",
    "workspace = \"dpv2-wks\"\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), sub_id, rg, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x000001E53A375120>,\n",
      "         subscription_id=b746917e-ceb7-4ae0-81e6-3ccd893cb0de,\n",
      "         resource_group_name=dpv2,\n",
      "         workspace_name=dpv2-wks)\n"
     ]
    }
   ],
   "source": [
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = load_component(path=\"./src/prepare_data.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "name: prepare_file_tabular_data\n",
      "version: '1'\n",
      "display_name: Prepare File Tabular Data\n",
      "type: command\n",
      "inputs:\n",
      "  input_data:\n",
      "    type: uri_folder\n",
      "outputs:\n",
      "  file_output_data:\n",
      "    type: mltable\n",
      "  tabular_output_data:\n",
      "    type: mltable\n",
      "command: python prepare_data.py --input_data ${{inputs.input_data}} --file_output_data\n",
      "  ${{outputs.file_output_data}} --tabular_output_data ${{outputs.tabular_output_data}}\n",
      "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:5\n",
      "code: ./\n",
      "is_deterministic: true\n",
      "tags: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prep_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "file_batch_inf = parallel_run_function(\n",
    "    name = \"file_batch_score\",\n",
    "    display_name=\"display: batch score with file dataset\",\n",
    "    description=\"description: batch score with file dataset\",\n",
    "    # metang: dict is just a way of to create inputs. input has 3 types of signatures. \n",
    "    # metang, dict to create the inputs using (one=1, two=2) format\n",
    "    # metang, below two format are equal\n",
    "    # method 1\n",
    "    # inputs={\n",
    "    # \"job_data_path\": Input(\n",
    "    #        type=AssetTypes.MLTABLE,\n",
    "    #        description=\"non dict approach create input\"\n",
    "    #    ),\n",
    "    # }\n",
    "    # method 2\n",
    "    # inputs=dict(\n",
    "    #    job_data_path=Input(\n",
    "    #        type=AssetTypes.MLTABLE,\n",
    "    #        description=\"the data to be split and scored in parallel\",\n",
    "    #    )\n",
    "    # )\n",
    "    inputs=dict(  \n",
    "        job_data_path=Input(\n",
    "            # metang: Is here file data type also as MLTable. What's the difference between this parallel step and the tabluar batch step below\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"the data to be split and scored in parallel\",\n",
    "        )\n",
    "    ),\n",
    "    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n",
    "    #outputs={\n",
    "    #    \"job_output_path\" : Output(\n",
    "    #        type = AssetTypes.MLTABLE\n",
    "    #    )\n",
    "    #},\n",
    "    input_data=\"${{inputs.job_data_path}}\",\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=1,\n",
    "    mini_batch_size=\"1\",\n",
    "    mini_batch_error_threshold=1,\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    logging_level=\"DEBUG\",\n",
    "    task=RunFunction(\n",
    "        code = \"./src\",\n",
    "        # metang: why we use entry_script, instead of like how we define commmand, just use command=\"python ...\"?\n",
    "        entry_script=\"file_batch_inference.py\",\n",
    "        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n",
    "        environment=\"azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tabular_batch_inf = parallel_run_function(\n",
    "    name = \"batch_score_with_tabular_input\",\n",
    "    display_name=\"display: Batch Score with Tabular Dataset\",\n",
    "    description=\"description: parallel component for batch score\",\n",
    "    inputs=dict(\n",
    "        job_data_path=Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"description: the data to be split and scored\",\n",
    "        ),\n",
    "        score_model = Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"description: input model\"\n",
    "        ),\n",
    "    ),\n",
    "    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n",
    "    # metang: is input_data a fixed input\n",
    "    input_data=\"${{inputs.job_data_path}}\", # i was using input_data_path, and caused \"\"evalue\": \"(UserError) Value cannot be null. \"\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=\"100\",\n",
    "    mini_batch_error_threshold=5,\n",
    "    logging_level=\"DEBUG\",\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    task=RunFunction(\n",
    "        code = \"./src\",\n",
    "        entry_script=\"tabular_batch_inference.py\",\n",
    "        environment=Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "            conda_file=\"./src/environment_parallel.yml\",\n",
    "        ),\n",
    "        append_row_to=\"${{outputs.job_output_path}}\",\n",
    "        program_arguments=\"--model ${{inputs.score_model}} \"\n",
    "        \"--job_output_path ${{outputs.job_output_path}} \"\n",
    "        # metang: not understand why we need use --error_threshold 5 here again. why do we need define mini_batch_error_threshold=5 in code 4 lines above, not in program_argumentics here.\n",
    "        \"--error_threshold 5 \"\n",
    "        \"--allowed_failed_percent 30 \"\n",
    "        \"--task_overhead_timeout 1200 \"\n",
    "        \"--progress_update_timeout 600 \"\n",
    "        \"--first_task_creation_timeout 600 \"\n",
    "        \"--copy_logs_to_parent True \"\n",
    "        \"--resource_monitor_interva 20 \"  \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def parallel_in_pipeline(job_data_path, score_model_path):\n",
    "\n",
    "    # step 1: prep_data\n",
    "    prep_data_step = prep_data(input_data=job_data_path)\n",
    "    # metang: why we need define the output type again. This is already defined in the component yaml?\n",
    "    prep_data_step.outputs.file_output_data.type = AssetTypes.MLTABLE\n",
    "    prep_data_step.outputs.tabular_output_data.type = AssetTypes.MLTABLE\n",
    "\n",
    "    # step 2: batch process data (prs)\n",
    "    batch_inf_file_step = file_batch_inf(\n",
    "        job_data_path = prep_data_step.outputs.file_output_data\n",
    "    )\n",
    "    # metang: it's interesting to see that define the input type is not in the prs constructor parallel_run_function. That function expose the input, but the actual input needs to be defined the type in the pipeline constructor\n",
    "    batch_inf_file_step.inputs.job_data_path.mode = (\n",
    "        InputOutputModes.EVAL_MOUNT\n",
    "    )\n",
    "    batch_inf_file_step.outputs.job_output_path.type = AssetTypes.MLTABLE\n",
    "\n",
    "    # step 3: batcn inferencing (prs)\n",
    "    batch_inf_tabular_step = tabular_batch_inf(\n",
    "        job_data_path = prep_data_step.outputs.tabular_output_data,\n",
    "        score_model=score_model_path,        \n",
    "    )\n",
    "    # metang: direct mode mean using data asset id, isn't it?\n",
    "    batch_inf_tabular_step.inputs.job_data_path.mode = (\n",
    "        InputOutputModes.DIRECT\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"job_output_file\": batch_inf_file_step.outputs.job_output_path,\n",
    "        \"job_output_tabular\": batch_inf_tabular_step.outputs.job_output_path,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_path = Input(\n",
    "    path=\"./dataset/\", type=AssetTypes.MLTABLE, mode=InputOutputModes.RO_MOUNT\n",
    ")\n",
    "score_model_path = Input(\n",
    "    path=\"./model/\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "job = parallel_in_pipeline(job_data_path=job_data_path, score_model_path=score_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metang: why here we dond define job.outputs.job_output_file type, but only define job.outputs.job_output_tabular??\n",
    "job.outputs.job_output_tabular.type = AssetTypes.URI_FILE\n",
    "\n",
    "job.settings.default_compute = \"cpu-cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_job = ml_client.jobs.create_or_update(job, experiment_name=\"prs_demo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/sleepy_oxygen_dq2jbnb1gz?wsid=/subscriptions/b746917e-ceb7-4ae0-81e6-3ccd893cb0de/resourcegroups/dpv2/workspaces/dpv2-wks&tid=9c37dc6f-e051-496f-832c-e4fcf894c611'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credential ok\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dpv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd93e304daff9e4e686bfc9607973de377fed28736a52eb30bf8c94fedb78ed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
