{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml import command, Input, Output, load_component\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = \"b746917e-ceb7-4ae0-81e6-3ccd893cb0de\"\n",
    "rg = \"dpv2\"\n",
    "workspace = \"dpv2-wks\"\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), sub_id, rg, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x000001C25B8A72E0>,\n",
      "         subscription_id=b746917e-ceb7-4ae0-81e6-3ccd893cb0de,\n",
      "         resource_group_name=dpv2,\n",
      "         workspace_name=dpv2-wks)\n"
     ]
    }
   ],
   "source": [
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = load_component(path=\"./src/prepare_data.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "name: prepare_file_tabular_data\n",
      "version: '1'\n",
      "display_name: Prepare File Tabular Data\n",
      "type: command\n",
      "inputs:\n",
      "  input_data:\n",
      "    type: uri_folder\n",
      "outputs:\n",
      "  file_output_data:\n",
      "    type: mltable\n",
      "  tabular_output_data:\n",
      "    type: mltable\n",
      "command: python prepare_data.py --input_data ${{inputs.input_data}} --file_output_data\n",
      "  ${{outputs.file_output_data}} --tabular_output_data ${{outputs.tabular_output_data}}\n",
      "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:5\n",
      "code: ./\n",
      "tags: {}\n",
      "is_deterministic: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prep_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "file_batch_inf = parallel_run_function(\n",
    "    name = \"file_batch_score\",\n",
    "    display_name=\"display: batch score with file dataset\",\n",
    "    description=\"description: batch score with file dataset\",\n",
    "    # metang: dict is just a way of to create inputs. input has 3 types of signatures. \n",
    "    # metang, dict to create the inputs using (one=1, two=2) format\n",
    "    # metang, below two format are equal\n",
    "    # method 1\n",
    "    # inputs={\n",
    "    # \"job_data_path\": Input(\n",
    "    #        type=AssetTypes.MLTABLE,\n",
    "    #        description=\"non dict approach create input\"\n",
    "    #    ),\n",
    "    # }\n",
    "    # method 2\n",
    "    # inputs=dict(\n",
    "    #    job_data_path=Input(\n",
    "    #        type=AssetTypes.MLTABLE,\n",
    "    #        description=\"the data to be split and scored in parallel\",\n",
    "    #    )\n",
    "    # )\n",
    "    inputs=dict(  \n",
    "        job_data_path=Input(\n",
    "            # metang: Is here file data type also as MLTable. What's the difference between this parallel step and the tabluar batch step below\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"the data to be split and scored in parallel\",\n",
    "        )\n",
    "    ),\n",
    "    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n",
    "    #outputs={\n",
    "    #    \"job_output_path\" : Output(\n",
    "    #        type = AssetTypes.MLTABLE\n",
    "    #    )\n",
    "    #},\n",
    "    input_data=\"${{inputs.job_data_path}}\",\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=1,\n",
    "    mini_batch_size=\"1\",\n",
    "    mini_batch_error_threshold=1,\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    logging_level=\"DEBUG\",\n",
    "    task=RunFunction(\n",
    "        code = \"./src\",\n",
    "        # metang: why we use entry_script, instead of like how we define commmand, just use command=\"python ...\"?\n",
    "        entry_script=\"file_batch_inference.py\",\n",
    "        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n",
    "        environment=\"azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tabular_batch_inf = parallel_run_function(\n",
    "    name = \"batch_score_with_tabular_input\",\n",
    "    display_name=\"display: Batch Score with Tabular Dataset\",\n",
    "    description=\"description: parallel component for batch score\",\n",
    "    inputs=dict(\n",
    "        job_data_path=Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"description: the data to be split and scored\",\n",
    "        ),\n",
    "        score_model = Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"description: input model\"\n",
    "        ),\n",
    "    ),\n",
    "    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n",
    "    # metang: is input_data a fixed input\n",
    "    input_data=\"${{inputs.job_data_path}}\", # i was using input_data_path, and caused \"\"evalue\": \"(UserError) Value cannot be null. \"\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=\"100\",\n",
    "    mini_batch_error_threshold=5,\n",
    "    logging_level=\"DEBUG\",\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    task=RunFunction(\n",
    "        code = \"./src\",\n",
    "        entry_script=\"tabula_batch_inference.py\",\n",
    "        environment=Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "            conda_file=\"./src/environment_parallel.yml\",\n",
    "        ),\n",
    "        append_row_to=\"${{outputs.job_output_path}}\",\n",
    "        program_arguments=\"--model ${{inputs.score_model}} \"\n",
    "        \"--job_output_path ${{outputs.job_output_path}} \"\n",
    "        # metang: not understand why we need use --error_threshold 5 here again. why do we need define mini_batch_error_threshold=5 in code 4 lines above, not in program_argumentics here.\n",
    "        \"--error_threshold 5 \"\n",
    "        \"--allowed_failed_percent 30 \"\n",
    "        \"--task_overhead_timeout 1200 \"\n",
    "        \"--progress_update_timeout 600 \"\n",
    "        \"--first_task_creation_timeout 600 \"\n",
    "        \"--copy_logs_to_parent True \"\n",
    "        \"--resource_monitor_interva 20 \"  \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def parallel_in_pipeline(job_data_path, score_model_path):\n",
    "\n",
    "    # step 1: prep_data\n",
    "    prep_data_step = prep_data(input_data=job_data_path)\n",
    "    # metang: why we need define the output type again. This is already defined in the component yaml?\n",
    "    prep_data_step.outputs.file_output_data.type = AssetTypes.MLTABLE\n",
    "    prep_data_step.outputs.tabular_output_data.type = AssetTypes.MLTABLE\n",
    "\n",
    "    # step 2: batch process data (prs)\n",
    "    batch_inf_file_step = file_batch_inf(\n",
    "        job_data_path = prep_data_step.outputs.file_output_data\n",
    "    )\n",
    "    # metang: it's interesting to see that define the input type is not in the prs constructor parallel_run_function. That function expose the input, but the actual input needs to be defined the type in the pipeline constructor\n",
    "    batch_inf_file_step.inputs.job_data_path.mode = (\n",
    "        InputOutputModes.EVAL_MOUNT\n",
    "    )\n",
    "    batch_inf_file_step.outputs.job_output_path.type = AssetTypes.MLTABLE\n",
    "\n",
    "    # step 3: batcn inferencing (prs)\n",
    "    batch_inf_tabular_step = tabular_batch_inf(\n",
    "        job_data_path = prep_data_step.outputs.tabular_output_data,\n",
    "        score_model=score_model_path,        \n",
    "    )\n",
    "    # metang: direct mode mean using data asset id, isn't it?\n",
    "    batch_inf_tabular_step.inputs.job_data_path.mode = (\n",
    "        InputOutputModes.DIRECT\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"job_output_file\": batch_inf_file_step.outputs.job_output_path,\n",
    "        \"job_output_tabular\": batch_inf_tabular_step.outputs.job_output_path,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_path = Input(\n",
    "    path=\"./dataset/\", type=AssetTypes.MLTABLE, mode=InputOutputModes.RO_MOUNT\n",
    ")\n",
    "score_model_path = Input(\n",
    "    path=\"./model/\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "job = parallel_in_pipeline(job_data_path=job_data_path, score_model_path=score_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metang: why here we dond define job.outputs.job_output_file type, but only define job.outputs.job_output_tabular??\n",
    "job.outputs.job_output_tabular.type = AssetTypes.URI_FILE\n",
    "\n",
    "job.settings.default_compute = \"cpu-cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_job = ml_client.jobs.create_or_update(job, experiment_name=\"prs_demo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load component\n",
    "prepare_data = load_component(path=\"./src/prepare_data.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel task to process file data\n",
    "file_batch_inference = parallel_run_function(\n",
    "    name=\"file_batch_score\",\n",
    "    display_name=\"Batch Score with File Dataset\",\n",
    "    description=\"parallel component for batch score\",\n",
    "    inputs=dict(\n",
    "        job_data_path=Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"The data to be split and scored in parallel\",\n",
    "        )\n",
    "    ),\n",
    "    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n",
    "    input_data=\"${{inputs.job_data_path}}\",\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=1,\n",
    "    mini_batch_size=\"1\",\n",
    "    mini_batch_error_threshold=1,\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    logging_level=\"DEBUG\",\n",
    "    task=RunFunction(\n",
    "        code=\"./src\",\n",
    "        entry_script=\"file_batch_inference.py\",\n",
    "        program_arguments=\"--job_output_path ${{outputs.job_output_path}}\",\n",
    "        environment=\"azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel task to process tabular data\n",
    "tabular_batch_inference = parallel_run_function(\n",
    "    name=\"batch_score_with_tabular_input\",\n",
    "    display_name=\"Batch Score with Tabular Dataset\",\n",
    "    description=\"parallel component for batch score\",\n",
    "    inputs=dict(\n",
    "        job_data_path=Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"The data to be split and scored in parallel\",\n",
    "        ),\n",
    "        score_model=Input(\n",
    "            type=AssetTypes.URI_FOLDER, description=\"The model for batch score.\"\n",
    "        ),\n",
    "    ),\n",
    "    outputs=dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n",
    "    input_data=\"${{inputs.job_data_path}}\",\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=\"100\",\n",
    "    mini_batch_error_threshold=5,\n",
    "    logging_level=\"DEBUG\",\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    task=RunFunction(\n",
    "        code=\"./src\",\n",
    "        entry_script=\"tabular_batch_inference.py\",\n",
    "        environment=Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "            conda_file=\"./src/environment_parallel.yml\",\n",
    "        ),\n",
    "        program_arguments=\"--model ${{inputs.score_model}} \"\n",
    "        \"--job_output_path ${{outputs.job_output_path}} \"\n",
    "        \"--error_threshold 5 \"\n",
    "        \"--allowed_failed_percent 30 \"\n",
    "        \"--task_overhead_timeout 1200 \"\n",
    "        \"--progress_update_timeout 600 \"\n",
    "        \"--first_task_creation_timeout 600 \"\n",
    "        \"--copy_logs_to_parent True \"\n",
    "        \"--resource_monitor_interva 20 \",\n",
    "        append_row_to=\"${{outputs.job_output_path}}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline()\n",
    "def parallel_in_pipeline(pipeline_job_data_path, pipeline_score_model):\n",
    "\n",
    "    prepare_file_tabular_data = prepare_data(input_data=pipeline_job_data_path)\n",
    "    # output of file & tabular data should be type MLTable\n",
    "    prepare_file_tabular_data.outputs.file_output_data.type = AssetTypes.MLTABLE\n",
    "    prepare_file_tabular_data.outputs.tabular_output_data.type = AssetTypes.MLTABLE\n",
    "\n",
    "    batch_inference_with_file_data = file_batch_inference(\n",
    "        job_data_path=prepare_file_tabular_data.outputs.file_output_data\n",
    "    )\n",
    "    # use eval_mount mode to handle file data\n",
    "    batch_inference_with_file_data.inputs.job_data_path.mode = (\n",
    "        InputOutputModes.EVAL_MOUNT\n",
    "    )\n",
    "    batch_inference_with_file_data.outputs.job_output_path.type = AssetTypes.MLTABLE\n",
    "\n",
    "    batch_inference_with_tabular_data = tabular_batch_inference(\n",
    "        job_data_path=prepare_file_tabular_data.outputs.tabular_output_data,\n",
    "        score_model=pipeline_score_model,\n",
    "    )\n",
    "    # use direct mode to handle tabular data\n",
    "    batch_inference_with_tabular_data.inputs.job_data_path.mode = (\n",
    "        InputOutputModes.DIRECT\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_out_file\": batch_inference_with_file_data.outputs.job_output_path,\n",
    "        \"pipeline_job_out_tabular\": batch_inference_with_tabular_data.outputs.job_output_path,\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_job_data_path = Input(\n",
    "    path=\"./dataset/\", type=AssetTypes.MLTABLE, mode=InputOutputModes.RO_MOUNT\n",
    ")\n",
    "pipeline_score_model = Input(\n",
    "    path=\"./model/\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n",
    ")\n",
    "# create a pipeline\n",
    "pipeline_job = parallel_in_pipeline(\n",
    "    pipeline_job_data_path=pipeline_job_data_path,\n",
    "    pipeline_score_model=pipeline_score_model,\n",
    ")\n",
    "pipeline_job.outputs.pipeline_job_out_tabular.type = AssetTypes.URI_FILE\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"cpu-cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading dataset (0.09 MBs): 100%|##########| 85267/85267 [00:01<00:00, 43272.28it/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading model (0.0 MBs): 100%|##########| 2227/2227 [00:01<00:00, 1781.52it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_samples</td><td>gentle_pasta_2xmkwcg7zh</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/gentle_pasta_2xmkwcg7zh?wsid=/subscriptions/b746917e-ceb7-4ae0-81e6-3ccd893cb0de/resourcegroups/dpv2/workspaces/dpv2-wks&amp;tid=9c37dc6f-e051-496f-832c-e4fcf894c611\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'pipeline_job_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInput object at 0x000001C25ED90DC0>, 'pipeline_score_model': <azure.ai.ml.entities._job.pipeline._io.PipelineInput object at 0x000001C25ED90160>}, 'outputs': {'pipeline_job_out_file': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x000001C25ED90E20>, 'pipeline_job_out_tabular': <azure.ai.ml.entities._job.pipeline._io.PipelineOutput object at 0x000001C25ED90E50>}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001C25EDF7FA0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'parallel_in_pipeline', 'is_deterministic': True, 'inputs': {'pipeline_job_data_path': {'type': 'unknown'}, 'pipeline_score_model': {'type': 'unknown'}}, 'outputs': {'pipeline_job_out_file': {'type': 'unknown'}, 'pipeline_job_out_tabular': {'type': 'unknown'}}, 'yaml_str': None, 'other_parameter': {}, 'func': <function [component] parallel_in_pipeline at 0x000001C25EF8A0E0>, 'jobs': {'prepare_file_tabular_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'prepare_file_tabular_data', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001C25EDF71C0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'input_data': '${{parent.inputs.pipeline_job_data_path}}'}, 'job_outputs': {'file_output_data': {'type': 'mltable'}, 'tabular_output_data': {'type': 'mltable'}}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x000001C25EDF77C0>}, 'outputs': {'file_output_data': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x000001C25EDF7820>, 'tabular_output_data': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x000001C25EDF7850>}, 'component': 'azureml_anonymous:af5adc18-e6bd-44fb-a512-3d0beda62204', 'kwargs': {'services': None}, 'instance_id': '6b94a413-dcda-4684-94f9-06efb063f56b', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'batch_inference_with_file_data': Parallel({'init': False, 'type': 'parallel', 'status': None, 'log_files': None, 'name': 'batch_inference_with_file_data', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': 'd:\\\\code\\\\dpv2demo\\\\PRS', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001C25EDF7700>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Batch Score with File Dataset', 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'job_data_path': {'type': 'uri_folder', 'path': '${{parent.jobs.prepare_file_tabular_data.outputs.file_output_data}}', 'mode': 'eval_mount'}}, 'job_outputs': {'job_output_path': '${{parent.outputs.pipeline_job_out_file}}'}, 'inputs': {'job_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x000001C25EDF7C10>}, 'outputs': {'job_output_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x000001C25EDF7C40>}, 'component': 'azureml_anonymous:6f89ba79-8730-4fd8-8703-86784be175c0', 'kwargs': {}, 'instance_id': '072d4e20-c7ff-42e0-823d-1da3fdf09889', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'task': {'type': 'run_function', 'code': './src', 'entry_script': 'file_batch_inference.py', 'program_arguments': '--job_output_path ${{outputs.job_output_path}}', 'environment': 'AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1'}, 'mini_batch_size': 1, 'input_data': '${{inputs.job_data_path}}', 'retry_settings': {'timeout': 60, 'max_retries': 2}, 'logging_level': 'DEBUG', 'max_concurrency_per_instance': 1, 'error_threshold': None, 'mini_batch_error_threshold': 1, 'resources': {'instance_count': 2, 'properties': {}}, 'environment_variables': {}}), 'batch_inference_with_tabular_data': Parallel({'init': False, 'type': 'parallel', 'status': None, 'log_files': None, 'name': 'batch_inference_with_tabular_data', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': 'd:\\\\code\\\\dpv2demo\\\\PRS', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001C25EDF7D90>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Batch Score with Tabular Dataset', 'experiment_name': None, 'compute': None, 'services': None, 'job_inputs': {'job_data_path': {'type': 'uri_folder', 'path': '${{parent.jobs.prepare_file_tabular_data.outputs.tabular_output_data}}', 'mode': 'direct'}, 'score_model': '${{parent.inputs.pipeline_score_model}}'}, 'job_outputs': {'job_output_path': '${{parent.outputs.pipeline_job_out_tabular}}'}, 'inputs': {'job_data_path': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x000001C25EDF7E50>, 'score_model': <azure.ai.ml.entities._job.pipeline._io.PipelineInputBase object at 0x000001C25EDF7E80>}, 'outputs': {'job_output_path': <azure.ai.ml.entities._job.pipeline._io.PipelineOutputBase object at 0x000001C25EDF7EB0>}, 'component': 'azureml_anonymous:ce041e3d-a708-4e43-a5c5-e2ad82c9e65a', 'kwargs': {}, 'instance_id': 'bcbeac6d-7df8-4ea4-9d6b-83eff2098c04', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'task': {'type': 'run_function', 'code': './src', 'entry_script': 'tabular_batch_inference.py', 'program_arguments': '--model ${{inputs.score_model}} --job_output_path ${{outputs.job_output_path}} --error_threshold 5 --allowed_failed_percent 30 --task_overhead_timeout 1200 --progress_update_timeout 600 --first_task_creation_timeout 600 --copy_logs_to_parent True --resource_monitor_interva 20 ', 'append_row_to': '${{outputs.job_output_path}}', 'environment': {'name': 'CliV2AnonymousEnvironment', 'tags': {}, 'version': '359f3a98c77bd2672808c76233db075f', 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04', 'conda_file': {'name': 'prs-env', 'channels': ['conda-forge'], 'dependencies': ['python=3.7.6', 'pip', {'pip': ['mlflow', 'azureml-dataset-runtime[pandas,fuse]', 'azureml-telemetry', 'pandas', 'pillow', 'azureml-core', 'scikit-learn~=0.20.0', 'cloudpickle==1.1.1', 'tensorflow==1.15.2']}]}}}, 'mini_batch_size': 100, 'input_data': '${{inputs.job_data_path}}', 'retry_settings': {'timeout': 60, 'max_retries': 2}, 'logging_level': 'DEBUG', 'max_concurrency_per_instance': 2, 'error_threshold': None, 'mini_batch_error_threshold': 5, 'resources': {'instance_count': 2, 'properties': {}}, 'environment_variables': {}})}, 'job_types': {'command': 1, 'parallel': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 3}}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'gentle_pasta_2xmkwcg7zh', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/metang/dpv2demo.git', 'mlflow.source.git.branch': 'prs', 'mlflow.source.git.commit': 'a6503497872978af27d99a0b50f958078c0671d1', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.defaultComputeName': 'cpu-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/b746917e-ceb7-4ae0-81e6-3ccd893cb0de/resourceGroups/dpv2/providers/Microsoft.MachineLearningServices/workspaces/dpv2-wks/jobs/gentle_pasta_2xmkwcg7zh', 'Resource__source_path': None, 'base_path': 'd:\\\\code\\\\dpv2demo\\\\PRS', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000001C25C956F80>, 'serialize': <msrest.serialization.Serializer object at 0x000001C25ED90B50>, 'display_name': 'parallel_in_pipeline', 'experiment_name': 'pipeline_samples', 'compute': None, 'services': {'Tracking': <azure.ai.ml._restclient.v2022_06_01_preview.models._models_py3.JobService object at 0x000001C25EDF7A00>, 'Studio': <azure.ai.ml._restclient.v2022_06_01_preview.models._models_py3.JobService object at 0x000001C25EDF7A30>}, 'settings': <azure.ai.ml.entities._job.pipeline.pipeline_job_settings.PipelineJobSettings object at 0x000001C25EDF7EE0>, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_samples\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dpv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd93e304daff9e4e686bfc9607973de377fed28736a52eb30bf8c94fedb78ed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
